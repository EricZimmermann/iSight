{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# deep learning \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# custom helpers\n",
    "import trainer\n",
    "import metrics\n",
    "import process\n",
    "import models\n",
    "\n",
    "# for formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_DISTRIBUTION = False\n",
    "MODEL_ZOO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "epochs = 30\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 1e-5\n",
    "step_period = 15\n",
    "lr_decay = 0.95\n",
    "num_workers = 4\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided by \"The ISIC 2020 Challenge\". The dataset is annotated for binary classification of skin lesions for melanoma detection. More information may be found at \"https://challenge2020.isic-archive.com/\"\n",
    "\n",
    "Data is sources from 2000 patients and includes 33,126 dermoscopic training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPath(root, image_id):\n",
    "    return os.path.join(root, image_id + '.npy')\n",
    "\n",
    "def toLabel(key, mapping):\n",
    "    return mapping[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "ham_path = '/usr/local/faststorage/ezimmer/data/'\n",
    "\n",
    "# parse data\n",
    "metadata = pd.read_csv('./data/HAM/metadata.csv')\n",
    "headers = metadata.head()\n",
    "\n",
    "# verify data\n",
    "label_mapping = {label : idx for idx, label in enumerate(sorted(np.unique(metadata['dx'])))}\n",
    "data_file = {ID : {'image' : None, 'label' : None} for ID in metadata['image_id']}\n",
    "\n",
    "for ID in metadata['image_id']:\n",
    "    data_file[ID]['image'] = toPath(ham_path, ID)\n",
    "    data_file[ID]['label'] = toLabel(metadata[metadata['image_id'] == ID]['dx'].values[0], label_mapping)\n",
    "\n",
    "# get class distribution\n",
    "class_counts = {idx : 0 for idx in range(len(np.unique(metadata['dx'])))}\n",
    "for ID in data_file.keys():\n",
    "    class_counts[data_file[ID]['label']] += 1\n",
    "    \n",
    "ncls = len(class_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HAM 10000 Metadata\")\n",
    "print('------------------------------------------------------------------')\n",
    "print(headers, '\\n')\n",
    "print(\"Number of Classes\")\n",
    "print(ncls, '\\n')\n",
    "print('Unique Labels')\n",
    "print(label_mapping, '\\n')\n",
    "print(\"Class Balance\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_DISTRIBUTION: \n",
    "    ys, xs = [], []\n",
    "    for ID in data_file.keys():\n",
    "        img = cv.imread(data_file[ID]['image'])\n",
    "        y, x, _ = img.shape\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "    \n",
    "    print(\"Dataset Image Size Distribution\")\n",
    "    print(\"Num Patients\", len(ys), len(xs))\n",
    "    print(\"Unique Values\", np.unique(ys), np.unique(xs))\n",
    "    plt.scatter(y,x)\n",
    "    plt.title(\"Image dimensions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment, Dataset, Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced, partition train/validation/test by number of classes and then wrap in an oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate partitioned exp\n",
    "partitioned_data = process.generateExperiment(data_file, ncls)\n",
    "\n",
    "# generate datasets\n",
    "train_sets = [process.SkinSet(partitioned_data[cls]['train']) for cls in range(ncls)]\n",
    "val_sets = [process.SkinSet(partitioned_data[cls]['validation']) for cls in range(ncls)]\n",
    "test_set = process.SkinSet(partitioned_data['test'])\n",
    "\n",
    "# oversampler on training and validation\n",
    "train_set = process.Oversampler(train_sets)\n",
    "val_set = process.Oversampler(val_sets)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialized ResNet-50/18/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "        if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "if MODEL_ZOO: \n",
    "    \n",
    "    def generateModel(n_cls, init=None, device=None):\n",
    "        model = torchvision.models.resnet34(pretrained=False)\n",
    "        model.fc.out_features=n_cls\n",
    "\n",
    "        if init is not None:\n",
    "            model.apply(init)\n",
    "\n",
    "        if device is not None:\n",
    "            model = model.to(device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = generateModel(ncls, init_weights, device)\n",
    "    \n",
    "else:\n",
    "    basic_config = ([3, 32, 64, 128, 256], 1, True, 0.15, 7)\n",
    "    model = models.BasicCNN(*basic_config).to(device)\n",
    "    model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer: AdamW for super convergence and fast training\n",
    "\n",
    "Scheduler: Step LR decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_period, lr_decay)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric storage\n",
    "train_stats = metrics.Aggregator()\n",
    "val_stats = metrics.Aggregator()\n",
    "test_stats = metrics.Aggregator()\n",
    "\n",
    "# add stats\n",
    "train_stats.addStat('loss')\n",
    "val_stats.addStat('loss')\n",
    "train_stats.addStat('acc', accuracy_score)\n",
    "val_stats.addStat('acc', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = process.Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stat = 0\n",
    "best_model = None\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    # train\n",
    "    preds, labels, t_loss = trainer.train(model, criterion, optimizer, scheduler, train_loader, device)\n",
    "    train_stats.logStat('loss', (t_loss,))\n",
    "    train_stats.logStat('acc', (labels, preds))\n",
    "    \n",
    "    # validate\n",
    "    preds, labels, v_loss = trainer.train(model, criterion, optimizer, scheduler, val_loader, device)\n",
    "    val_stats.logStat('loss', (v_loss,))\n",
    "    val_stats.logStat('acc', (labels, preds))\n",
    "    \n",
    "    if val_stats.getStats('acc')[-1] > best_stat:\n",
    "        best_stat = val_stats.getStats('acc')[-1]\n",
    "        best_model = model.state_dict()\n",
    "    \n",
    "    t = time.time() - t\n",
    "        \n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Time:\", t)\n",
    "    print(\"Training Loss:       \", t_loss)\n",
    "    print(\"Validation Loss:     \", v_loss)\n",
    "    print(\"Training Accuracy:   \", train_stats.getStats('acc')[-1])\n",
    "    print(\"Validation Accuracy: \", val_stats.getStats('acc')[-1], '\\n')\n",
    "    \n",
    "    \n",
    "metrics.Plotter.plot(train_stats.getStats('loss'), val_stats.getStats('loss'), 'Epochs', 'Loss', 'Losses')\n",
    "metrics.Plotter.plot(train_stats.getStats('acc'), val_stats.getStats('acc'), 'Epochs', 'Accuracy', 'Accuracies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "preds, labels, _ = trainer.evaluate(model, criterion, optimizer, scheduler, test_loader, device)\n",
    "test_acc = accuracy_score(labels, preds)\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "plot_confusion_matrix(conf_mat, sorted(list(label_mapping.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
