{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ensemble For Uncertainty Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "# data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, balanced_accuracy_score\n",
    "import itertools\n",
    "\n",
    "# deep learning \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.transforms as txf\n",
    "import kornia.augmentation as k\n",
    "\n",
    "# custom helpers\n",
    "import trainer\n",
    "import metrics\n",
    "import process\n",
    "import models\n",
    "\n",
    "# for formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_DISTRIBUTION = False\n",
    "MODEL_ZOO = True\n",
    "MODEL_TYPE = [\"RES\", \"DENSE\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "epochs = 30\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 1e-5\n",
    "step_period = 3\n",
    "lr_decay = 0.95\n",
    "num_workers = 6\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided by \"HAM10000\" - 7 classes!\n",
    "\n",
    "\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\"\n",
    "\n",
    "    - AKIEC : Actinic keratoses and intraepithelial carcinoma / Bowen's disease \n",
    "    - BCC   : Basal cell carcinoma \n",
    "    - BKL.  : Benign keratosis-like lesions\n",
    "    - DF.   : Dermatofibroma \n",
    "    - MEL.  : Melanoma\n",
    "    - NV.   : Melanocytic nevi \n",
    "    - VASC  : Vascular lesions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPath(root, image_id):\n",
    "    return os.path.join(root, image_id + '.npy')\n",
    "\n",
    "def toLabel(key, mapping):\n",
    "    return mapping[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "ham_path = \"YOUR PATH HERE\"\n",
    "\n",
    "# parse data\n",
    "metadata = pd.read_csv('./data/HAM/metadata.csv')\n",
    "headers = metadata.head()\n",
    "\n",
    "# verify data\n",
    "label_mapping = {label : idx for idx, label in enumerate(sorted(np.unique(metadata['dx'])))}\n",
    "data_file = {ID : {'image' : None, 'label' : None} for ID in metadata['image_id']}\n",
    "\n",
    "for ID in metadata['image_id']:\n",
    "    data_file[ID]['image'] = toPath(ham_path, ID)\n",
    "    data_file[ID]['label'] = toLabel(metadata[metadata['image_id'] == ID]['dx'].values[0], label_mapping)\n",
    "\n",
    "# get class distribution\n",
    "class_counts = {idx : 0 for idx in range(len(np.unique(metadata['dx'])))}\n",
    "for ID in data_file.keys():\n",
    "    class_counts[data_file[ID]['label']] += 1\n",
    "    \n",
    "ncls = len(class_counts.keys())\n",
    "weights = torch.tensor([1 / val for val in class_counts.values()])*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HAM 10000 Metadata\")\n",
    "print('------------------------------------------------------------------')\n",
    "print(headers, '\\n')\n",
    "print(\"Number of Classes\")\n",
    "print(ncls, '\\n')\n",
    "print('Unique Labels')\n",
    "print(label_mapping, '\\n')\n",
    "print(\"Class Balance\")\n",
    "print(class_counts, '\\n')\n",
    "print(\"Inverse Weighting\")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_DISTRIBUTION: \n",
    "    ys, xs = [], []\n",
    "    for ID in data_file.keys():\n",
    "        img = cv.imread(data_file[ID]['image'])\n",
    "        y, x, _ = img.shape\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "    \n",
    "    print(\"Dataset Image Size Distribution\")\n",
    "    print(\"Num Patients\", len(ys), len(xs))\n",
    "    print(\"Unique Values\", np.unique(ys), np.unique(xs))\n",
    "    plt.scatter(y,x)\n",
    "    plt.title(\"Image dimensions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment, Dataset, Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentations = process.Transformer()\n",
    "train_augmentations.add(k.RandomHorizontalFlip())\n",
    "train_augmentations.add(k.RandomVerticalFlip())\n",
    "train_augmentations.add(k.RandomRotation(15))\n",
    "train_augmentations.add(k.ColorJitter(0.1, 0.1, 0.1))\n",
    "train_augmentations.add(txf.Lambda(lambda x: x.squeeze()))\n",
    "train_txf = train_augmentations.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced, partition train/validation/test by number of classes and then wrap in an oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate partitioned exp\n",
    "partitioned_data = process.generateExperiment(data_file, ncls)\n",
    "merged_data = process.restructureExperiments(partitioned_data)\n",
    "\n",
    "# generate datasets\n",
    "train_set = process.SkinSet(merged_data['train'])\n",
    "val_set = process.SkinSet(merged_data['validation'])\n",
    "test_set = process.SkinSet(merged_data['test'])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialized ResNet-50/18/34 or pretrained. Dense attempted but runtime is too slow :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "        if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "if MODEL_ZOO: \n",
    "    \n",
    "    def generateModel(n_cls, model_type, init=None, device=None):\n",
    "        if model_type == 'RES':\n",
    "            model = torchvision.models.resnet18(pretrained=True)\n",
    "            in_ftr  = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_ftr,n_cls,bias=True)\n",
    "        \n",
    "        elif model_type == 'DENSE':\n",
    "            model = torchvision.models.densenet121(pretrained=True)\n",
    "            model_lin = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(model_lin, n_cls)\n",
    "\n",
    "        if init is not None:\n",
    "            model.apply(init)\n",
    "\n",
    "        if device is not None:\n",
    "            model = model.to(device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = generateModel(ncls, model_type=MODEL_TYPE, init=None, device=device)\n",
    "    \n",
    "else:\n",
    "    basic_config = ([3, 32, 64, 128, 256], 1, True, 0.15, 7)\n",
    "    model = models.BasicCNN(*basic_config).to(device)\n",
    "    model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer: AdamW for super convergence and fast training\n",
    "\n",
    "Scheduler: Step LR decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_period, lr_decay)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric storage\n",
    "train_stats = metrics.Aggregator()\n",
    "val_stats = metrics.Aggregator()\n",
    "\n",
    "# add stats\n",
    "train_stats.addStat('loss')\n",
    "val_stats.addStat('loss')\n",
    "train_stats.addStat('acc', accuracy_score)\n",
    "val_stats.addStat('acc', accuracy_score)\n",
    "train_stats.addStat('bal', balanced_accuracy_score)\n",
    "val_stats.addStat('bal', balanced_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_stat = 0\n",
    "best_model = None\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    # train\n",
    "    preds, labels, t_loss = trainer.train(model, criterion, optimizer, scheduler, train_loader, device, train_txf)\n",
    "    train_stats.logStat('loss', (t_loss,))\n",
    "    train_stats.logStat('acc', (labels, preds))\n",
    "    train_stats.logStat('bal', (labels, preds))\n",
    "    \n",
    "    # validate\n",
    "    preds, labels, v_loss = trainer.evaluate(model, criterion, optimizer, scheduler, val_loader, device)\n",
    "    val_stats.logStat('loss', (v_loss,))\n",
    "    val_stats.logStat('acc', (labels, preds))\n",
    "    val_stats.logStat('bal', (labels, preds))\n",
    "    \n",
    "    if val_stats.getStats('bal')[-1] > best_stat:\n",
    "        best_stat = val_stats.getStats('bal')[-1]\n",
    "        best_model = model.state_dict()\n",
    "    \n",
    "    t = time.time() - t\n",
    "        \n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Time:\", t)\n",
    "    print(\"Training Loss:       \", t_loss)\n",
    "    print(\"Validation Loss:     \", v_loss)\n",
    "    print(\"Training Accuracy:   \", train_stats.getStats('acc')[-1])\n",
    "    print(\"Validation Accuracy: \", val_stats.getStats('acc')[-1])\n",
    "    print(\"Training Balanced:   \", train_stats.getStats('bal')[-1])\n",
    "    print(\"Validation Balanced: \", val_stats.getStats('bal')[-1], '\\n')\n",
    "    \n",
    "    \n",
    "metrics.Plotter.plot(train_stats.getStats('loss'), val_stats.getStats('loss'), 'Epochs', 'Loss', 'Losses')\n",
    "metrics.Plotter.plot(train_stats.getStats('acc'), val_stats.getStats('acc'), 'Epochs', 'Accuracy', 'Accuracies')\n",
    "metrics.Plotter.plot(train_stats.getStats('bal'), val_stats.getStats('bal'), 'Epochs', 'Accuracy', 'Balanced Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./balanced_ensembles'):\n",
    "    os.mkdir('./balanced_ensembles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('balanced_exp.pickle', 'wb') as handle:\n",
    "    pickle.dump(merged_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, os.path.join('./balanced_ensembles', 'model_' + str(0) + '.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join('./balanced_ensembles', 'model_' + str(0) + '.pth.tar')))\n",
    "preds, labels, _ = trainer.evaluate(model, criterion, optimizer, scheduler, test_loader, device)\n",
    "test_acc = accuracy_score(labels, preds)\n",
    "test_bal = balanced_accuracy_score(labels, preds)\n",
    "conf_mat = confusion_matrix(labels, preds)\n",
    "print(\"Test Accuracy: \", test_acc)\n",
    "print(\"Test Balance: \", test_bal)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ensembles = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(n_ensembles):\n",
    "    model = generateModel(ncls, MODEL_TYPE, None, device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_period, lr_decay)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "    train_stats = metrics.Aggregator()\n",
    "    val_stats = metrics.Aggregator()\n",
    "    train_stats.addStat('loss')\n",
    "    val_stats.addStat('loss')\n",
    "    train_stats.addStat('acc', accuracy_score)\n",
    "    val_stats.addStat('acc', accuracy_score)\n",
    "    train_stats.addStat('bal', balanced_accuracy_score)\n",
    "    val_stats.addStat('bal', balanced_accuracy_score)\n",
    "    \n",
    "    best_stat = 0\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # train\n",
    "        preds, labels, t_loss = trainer.train(model, criterion, optimizer, scheduler, train_loader, device, train_txf)\n",
    "        train_stats.logStat('loss', (t_loss,))\n",
    "        train_stats.logStat('acc', (labels, preds))\n",
    "        train_stats.logStat('bal', (labels, preds))\n",
    "\n",
    "        # validate\n",
    "        preds, labels, v_loss = trainer.evaluate(model, criterion, optimizer, scheduler, val_loader, device)\n",
    "        val_stats.logStat('loss', (v_loss,))\n",
    "        val_stats.logStat('acc', (labels, preds))\n",
    "        val_stats.logStat('bal', (labels, preds))\n",
    "\n",
    "        if val_stats.getStats('bal')[-1] > best_stat:\n",
    "            best_stat = val_stats.getStats('bal')[-1]\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        t = time.time() - t\n",
    "        \n",
    "        print(\"MODEL:\", idx+1)\n",
    "        print(\"Epoch:\", epoch+1)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Time:\", t)\n",
    "        print(\"Training Loss:       \", t_loss)\n",
    "        print(\"Validation Loss:     \", v_loss)\n",
    "        print(\"Training Accuracy:   \", train_stats.getStats('acc')[-1])\n",
    "        print(\"Validation Accuracy: \", val_stats.getStats('acc')[-1])\n",
    "        print(\"Training Balanced:   \", train_stats.getStats('bal')[-1])\n",
    "        print(\"Validation Balanced: \", val_stats.getStats('bal')[-1], '\\n')\n",
    "        \n",
    "    torch.save(best_model, os.path.join('./balanced_ensembles', 'model_' + str(idx+1) + '.pth.tar'))                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
