{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# deep learning \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# custom helpers\n",
    "import trainer\n",
    "import metrics\n",
    "import process\n",
    "\n",
    "# for formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_DISTRIBUTION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "epochs = 100\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 1e-4\n",
    "step_period = 15\n",
    "lr_decay = 0.95\n",
    "num_workers = 4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided by \"The ISIC 2020 Challenge\". The dataset is annotated for binary classification of skin lesions for melanoma detection. More information may be found at \"https://challenge2020.isic-archive.com/\"\n",
    "\n",
    "Data is sources from 2000 patients and includes 33,126 dermoscopic training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPath(root, image_id):\n",
    "    return os.path.join(root, image_id + '.npy')\n",
    "\n",
    "def toLabel(key, mapping):\n",
    "    return mapping[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "ham_path = '/usr/local/faststorage/ezimmer/data/'\n",
    "\n",
    "# parse data\n",
    "metadata = pd.read_csv('./data/HAM/metadata.csv')\n",
    "headers = metadata.head()\n",
    "\n",
    "# verify data\n",
    "label_mapping = {label : idx for idx, label in enumerate(sorted(np.unique(metadata['dx'])))}\n",
    "data_file = {ID : {'image' : None, 'label' : None} for ID in metadata['image_id']}\n",
    "\n",
    "for ID in metadata['image_id']:\n",
    "    data_file[ID]['image'] = toPath(ham_path, ID)\n",
    "    data_file[ID]['label'] = toLabel(metadata[metadata['image_id'] == ID]['dx'].values[0], label_mapping)\n",
    "\n",
    "# get class distribution\n",
    "class_counts = {idx : 0 for idx in range(len(np.unique(metadata['dx'])))}\n",
    "for ID in data_file.keys():\n",
    "    class_counts[data_file[ID]['label']] += 1\n",
    "    \n",
    "ncls = len(class_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM 10000 Metadata\n",
      "------------------------------------------------------------------\n",
      "     lesion_id      image_id   dx dx_type   age   sex localization\n",
      "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
      "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
      "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
      "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
      "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear \n",
      "\n",
      "Number of Classes\n",
      "7 \n",
      "\n",
      "Unique Labels\n",
      "{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6} \n",
      "\n",
      "Class Balance\n",
      "{0: 327, 1: 514, 2: 1099, 3: 115, 4: 1113, 5: 6705, 6: 142}\n"
     ]
    }
   ],
   "source": [
    "print(\"HAM 10000 Metadata\")\n",
    "print('------------------------------------------------------------------')\n",
    "print(headers, '\\n')\n",
    "print(\"Number of Classes\")\n",
    "print(ncls, '\\n')\n",
    "print('Unique Labels')\n",
    "print(label_mapping, '\\n')\n",
    "print(\"Class Balance\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_DISTRIBUTION: \n",
    "    ys, xs = [], []\n",
    "    for ID in data_file.keys():\n",
    "        img = cv.imread(data_file[ID]['image'])\n",
    "        y, x, _ = img.shape\n",
    "        ys.append(y)\n",
    "        xs.append(x)\n",
    "    \n",
    "    print(\"Dataset Image Size Distribution\")\n",
    "    print(\"Num Patients\", len(ys), len(xs))\n",
    "    print(\"Unique Values\", np.unique(ys), np.unique(xs))\n",
    "    plt.scatter(y,x)\n",
    "    plt.title(\"Image dimensions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment, Dataset, Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced, partition train/validation/test by number of classes and then wrap in an oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate partitioned exp\n",
    "partitioned_data = process.generateExperiment(data_file, ncls)\n",
    "\n",
    "# generate datasets\n",
    "train_sets = [process.SkinSet(partitioned_data[cls]['train']) for cls in range(ncls)]\n",
    "val_sets = [process.SkinSet(partitioned_data[cls]['validation']) for cls in range(ncls)]\n",
    "test_set = process.SkinSet(partitioned_data['test'])\n",
    "\n",
    "# oversampler on training and validation\n",
    "train_set = process.Oversampler(train_sets)\n",
    "val_set = process.Oversampler(val_sets)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialized ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModel(init=None, device=None):\n",
    "    model = torchvision.models.resnet50(pretrained=False)\n",
    "    model.fc.out_features=2\n",
    "        \n",
    "    if init is not None:\n",
    "        model.apply(init)\n",
    "        \n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def init_weights(m):\n",
    "    if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    \n",
    "model = generateModel(init_weights, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer: AdamW for super convergence and fast training\n",
    "\n",
    "Scheduler: Step LR decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_period, lr_decay)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric storage\n",
    "train_stats = metrics.Aggregator()\n",
    "val_stats = metrics.Aggregator()\n",
    "test_stats = metrics.Aggregator()\n",
    "\n",
    "# add stats\n",
    "train_stats.addStat('loss')\n",
    "val_stats.addStat('loss')\n",
    "train_stats.addStat('acc', accuracy_score)\n",
    "val_stats.addStat('acc', accuracy_score)\n",
    "test_stats.addStat('acc', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "--------------------------------\n",
      "Time: 35.20189571380615\n",
      "Training Loss:        1.1452278\n",
      "Validation Loss:      3.699743\n",
      "Training Accuracy:    0.5826759061833688\n",
      "Validation Accuracy:  0.655223880597015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15e45fc167b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mval_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogStat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mval_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogStat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/data/ezimmer/implementAI2020/dl/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, loader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/data/ezimmer/imp/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/data/ezimmer/imp/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/data/ezimmer/imp/lib/python3.6/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_stat = 0\n",
    "best_model = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    # train\n",
    "    preds, labels, t_loss = trainer.train(model, criterion, optimizer, scheduler, train_loader, device)\n",
    "    train_stats.logStat('loss', (t_loss,))\n",
    "    train_stats.logStat('acc', (labels, preds))\n",
    "    \n",
    "    # validate\n",
    "    preds, labels, v_loss = trainer.train(model, criterion, optimizer, scheduler, val_loader, device)\n",
    "    val_stats.logStat('loss', (v_loss,))\n",
    "    val_stats.logStat('acc', (labels, preds))\n",
    "    \n",
    "    if val_stats.getStats('acc')[-1] > best_stat:\n",
    "        best_stat = val_stats.getStats('acc')[-1]\n",
    "        best_model = model.state_dict()\n",
    "    \n",
    "    t = time.time() - t\n",
    "        \n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Time:\", t)\n",
    "    print(\"Training Loss:       \", t_loss)\n",
    "    print(\"Validation Loss:     \", v_loss)\n",
    "    print(\"Training Accuracy:   \", train_stats.getStats('acc')[-1])\n",
    "    print(\"Validation Accuracy: \", val_stats.getStats('acc')[-1])\n",
    "    \n",
    "    \n",
    "metrics.Plotter.plot(train_stats.getStats('loss'), val_stats.getStats('loss'), 'Epochs, Loss', 'Losses')\n",
    "metrics.Plotter.plot(train_stats.getStats('acc'), val_stats.getStats('acc'), 'Epochs, Accuracy', 'Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
